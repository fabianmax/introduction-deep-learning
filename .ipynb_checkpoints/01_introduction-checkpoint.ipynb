{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Introduction to Deep Learning with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Chapter 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.1 Rosenblatt's Perceptron\n",
    "\n",
    "The **perceptron** was the first type of a artificial neuron introduced by [Frank Rosenblatt][1] in the late 1950. It's design was inspired by the McCulloch-Pitts model of a neuron. While perceptrons nowadays were replaced by other types of neurons, their basic design continues to exist in modern neural networks. \n",
    "\n",
    "A perceptron can be used to learn a *linearly separable* classification task. It takes **inputs** ${[x_1, x_2, ..., x_n]}$ and computes a binary output $y_i$. The **weights** ${[w_1, w_2, ..., w_n]}$ express the importance of the respective inputs to the output. The output is calculated as a weightes sum over the inputs:\n",
    "\n",
    "$$y_i = \\sum_{i}w_i x_i$$\n",
    "\n",
    "[1]: http://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "somecaption",
    "deletable": true,
    "editable": true,
    "label": "fig:somelabel",
    "widefigure": true
   },
   "source": [
    "![Perceptron](00_ressources/img/chapter_1/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In order to ensure that $y_i$ is a binary outcome, the perceptron uses a *step-function* (*hard limiter*) with an estimated *threshold* also called **bias**:\n",
    "\n",
    "$$y_i = \n",
    "\\begin{cases}\n",
    "    0 &\\text{if $w \\cdot x+b \\leq 0$}\\cr  \n",
    "    1 &\\text{if $w \\cdot x+b \\geq 0$}\n",
    "\\end{cases}$$\n",
    "\n",
    "where $w \\cdot x \\equiv \\sum_{i}w_i x_i$ is the *dot product* between $x$ and $w$ and $b$ is the threshold. The step-function is a nonlinear function which maps the weighted sum to the desired output. Later we will see, that while modern neuronal networks still require a nonlinear function, their shape is somehow more smooth compared to the step-function (*soft limiter*). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "caption": "somecaption",
    "deletable": true,
    "editable": true,
    "label": "fig:somelabel",
    "widefigure": true
   },
   "source": [
    "![Stepfunction](00_ressources/img/chapter_1/step_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Finally, the perceptron learns by iteratively updating the weight vector $w$ in the following way:\n",
    "\n",
    "$$w \\leftarrow \\dot{w} + \\nu \\cdot (y_i - \\hat{y_i}) \\cdot x_i $$\n",
    "\n",
    "where $\\dot{w}$ is the new weight vector, $\\nu$ is a *learning rate*, $(y_i - \\hat{y_i})$ is the error in the current iteration and is the current input $x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: -0.113 -> 0 | 0\n",
      "1: 0.0 -> 1 | 1\n",
      "2: 0.326 -> 1 | 1\n",
      "3: 0.439 -> 1 | 1\n"
     ]
    }
   ],
   "source": [
    "# Coding Rosenblatt's Perceptron from scratch\n",
    "# -------------------------------------------\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "# Step function\n",
    "def unit_step(x):\n",
    "    if x < 0:\n",
    "        return(0)\n",
    "    else:\n",
    "        return(1)\n",
    "\n",
    "# Data\n",
    "X = np.array([[0,0,1], \n",
    "              [0,1,1], \n",
    "              [1,0,1], \n",
    "              [1,1,1]]\n",
    "            )\n",
    "# Label\n",
    "y = np.array([0,1,1,1])\n",
    "\n",
    "w = np.random.rand(3) # Weights\n",
    "errors = []           # Errors\n",
    "eta = 0.2             # Learning rate\n",
    "n = 100               # Epochs\n",
    "\n",
    "# Training\n",
    "for i in range(n):\n",
    "    # Get row index\n",
    "    index = random.randint(0,3)\n",
    "    # Define minibatch (online)\n",
    "    x_batch = X[index,:]\n",
    "    y_batch = y[index]\n",
    "    # Calculate activation\n",
    "    y_hat = unit_step(np.dot(w, x_batch))\n",
    "    # Caluclate error\n",
    "    error = y_batch - y_hat\n",
    "    errors.append(error)\n",
    "    # Update weights\n",
    "    w += eta * error * x_batch\n",
    "\n",
    "# Prediction  \n",
    "for index, x in enumerate(X):\n",
    "    y_hat = np.dot(x, w)\n",
    "    print(\"{}: {} -> {} | {}\".format(index, round(y_hat, 3), unit_step(y_hat), y[index]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.2 Limitations of Perceptrons\n",
    "\n",
    "Shortly after the first publication, the perceptron gained a lot of attention in the early 1960s and was generally considered as a very powerful learning algorithmen. This perspective rapidly switched in the late 1960s and early 1970s after the famous critique by [Minsky and Papert (1969)][2].\n",
    "\n",
    "Minsky and Papert proved that the perceptron was actually quite limited in what it can learn. More precise, they showed that the perceptron requires the right features in order to learn a classification task correctly. Given enough hand-selected features the perceptron performance still very well. \n",
    "\n",
    "[2]: https://mitpress.mit.edu/books/perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "[Source][3]: \n",
    "> Once upon a time, the US Army wanted to use neural networks to automatically detect camouflaged enemy tanks. The researchers trained a neural net on 50 photos of camouflaged tanks in trees, and 50 photos of trees without tanks. Using standard techniques for supervised learning, the researchers trained the neural network to a weighting that correctly loaded the training set—output “yes” for the 50 photos of camouflaged tanks, and output “no” for the 50 photos of forest.  is did not ensure, or even imply, that new examples would be classified correctly. \n",
    "\n",
    "> The neural network might have “learned” 100 special cases that would not generalize to any new problem. Wisely, the researchers had originally taken 200 photos, 100 photos of tanks and 100 photos of trees. They had used only 50 of each for the training set.  The researchers ran the neural network on the remaining 100 photos, and without further training the neural network classified all remaining photos correctly. \n",
    "Success confirmed! The researchers handed the  finished work to the Pentagon, which soon handed it back, complaining that in their own tests the neural network did no better than chance at discriminating photos.\n",
    "\n",
    ">It turned out that in the researchers’ dataset, photos of camouflaged tanks had been taken on cloudy days, while photos of plain forest had been taken on sunny days. The neural network had learned to distinguish cloudy days from sunny days, instead of distinguishing camouflaged tanks from empty forest.\n",
    "\n",
    "[3]: http://intelligence.org/files/AIPosNegFactor.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.087 -> 1 | 0\n",
      "1: 0.0 -> 1 | 1\n",
      "2: -0.074 -> 0 | 1\n",
      "3: -0.161 -> 0 | 0\n"
     ]
    }
   ],
   "source": [
    "# Same data\n",
    "X = np.array([[0,0,1], \n",
    "              [0,1,1], \n",
    "              [1,0,1], \n",
    "              [1,1,1]]\n",
    "            )\n",
    "\n",
    "# Updated label\n",
    "y = np.array([1,0,0,1])\n",
    "\n",
    "# Again training\n",
    "for i in range(n):\n",
    "    # Get row index\n",
    "    index = random.randint(0,3)\n",
    "    # Define minibatch (online)\n",
    "    x_batch = X[index,:]\n",
    "    y_batch = y[index]\n",
    "    # Calculate activation\n",
    "    y_hat = unit_step(np.dot(w, x_batch))\n",
    "    # Caluclate error\n",
    "    error = y_batch - y_hat\n",
    "    errors.append(error)\n",
    "    # Update weights\n",
    "    w += eta * error * x_batch\n",
    "\n",
    "# ... and prediction  \n",
    "for index, x in enumerate(X):\n",
    "    y_hat = np.dot(x, w)\n",
    "    print(\"{}: {} -> {} | {}\".format(index, round(y_hat, 3), unit_step(y_hat), y[index]))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Given four observations we obtain the following ineuqualities:\n",
    "\n",
    "$$w_1 * 0 + w_2 * 0 = 0 \\geq b$$\n",
    "$$w_1 * 0 + w_2 * 1 = w_1 < b$$\n",
    "$$w_1 * 1 + w_2 * 0 = w_2 < b$$\n",
    "$$w_1 * 1 + w_2 * 1 = w_1 + w_2 \\geq b$$  \n",
    "\n",
    "Which can be added up as:\n",
    "\n",
    "$$w_1 + w_2 \\geq 2b$$\n",
    "$$w_1 + w_2 < 2b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Unfortunately Minsky and Papert's critique was misunderstood by a large portion of the scientific community: If learning the right features is the essential part and neuronal networks can not learn those features by them selfs, they are generally useless for non-trivial learning problems.    \n",
    "\n",
    "While this unterstanding is in general not true, it emerged as a common wisdom holding for the next 20 years while\n",
    "leading to a dramatic decrease in scientific interest in neural networks for learning problems. It was not before the late 1980s and early 1990s, when people started to realize that learning the features resolves this limitation.  \n",
    "\n",
    "Recognizing that learning good features is the real problem, not only affected the further neural network research but also gave rise to other learning algorithms. One of the most prominent ones is the *Support Vector Machine (SVM)*, where the learning problem is solved by transforming the input space into a nonlinear feature space. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
